{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.model.base import Base\n",
    "from lib.utils import set_seed,sample_indices,get_experiment_dir\n",
    "from lib.augmentations import apply_augmentations\n",
    "from lib.distance.w1_distance import W1_distance,W1_dist_PathSpace,W1_dist_SigSpace\n",
    "from lib.datasets import get_stock_price, train_test_split\n",
    "from lib.network import get_generator, get_discriminator\n",
    "from lib.test_metrics import get_standard_test_metrics\n",
    "from lib.plot import plot_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "wgan_config = {\n",
    "    'batch_size' : 1024,\n",
    "    'lr_generator' : 5e-4,\n",
    "    'lr_discriminator' : 5e-4,\n",
    "    'discriminator_steps_per_generator_step' : 3,\n",
    "    'device' : 'cuda:0' if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "data_config = {\n",
    "    'ticker' : '^GSPC',\n",
    "    'interval' : '1mo',\n",
    "    'column' : 0,  #Open\n",
    "    'window_size' : 3,\n",
    "    'dir' : 'datasets',\n",
    "    'subdir' : 'stock',\n",
    "}\n",
    "D_config = {\n",
    "    \"discriminator_type\": \"ResFNN\",\n",
    "    \"hidden_dims\": [20,20],\n",
    "}\n",
    "G_config = {\n",
    "    \"generator_type\": \"LSTM\",\n",
    "    \"hidden_dim\": 50,\n",
    "    \"n_layers\": 2,\n",
    "    \"init_fixed\": True,\n",
    "    \"input_dim\": 5\n",
    "}\n",
    "print(wgan_config['device'])\n",
    "\n",
    "set_seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolled data for training, shape torch.Size([286, 3, 1])\n",
      "Total data:  [286, 3, 1]\n",
      "D input dim: 3\n",
      "G output dim: 1\n"
     ]
    }
   ],
   "source": [
    "x_real_rolled = get_stock_price(data_config)\n",
    "x_real_rolled = x_real_rolled.to(wgan_config['device'])\n",
    "print('Total data: ', list(x_real_rolled.shape))\n",
    "\n",
    "x_real_train, x_real_test = train_test_split(x_real_rolled, train_test_ratio=0.8, device=wgan_config['device'])\n",
    "x_real_dim: int = x_real_rolled.shape[2]\n",
    "wgan_input_dim = x_real_dim * data_config['window_size']\n",
    "\n",
    "print(\"D input dim: {}\".format(wgan_input_dim)) # D output dim is always 1\n",
    "print(\"G output dim: {}\".format(x_real_dim)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(model, requires_grad):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANTrainer(Base):\n",
    "    def __init__(self, D, G, discriminator_steps_per_generator_step,\n",
    "                 lr_discriminator, lr_generator, x_real: torch.Tensor, reg_param=10.,\n",
    "                 **kwargs):\n",
    "        if kwargs.get('augmentations') is not None:\n",
    "            self.augmentations = kwargs['augmentations']\n",
    "            del kwargs['augmentations']\n",
    "        else:\n",
    "            self.augmentations = None\n",
    "        super(WGANTrainer, self).__init__(\n",
    "            G=G,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.D_steps_per_G_step = discriminator_steps_per_generator_step\n",
    "        self.D = D\n",
    "        self.D_optimizer = torch.optim.Adam(D.parameters(), lr=lr_discriminator, betas=(0, 0.9))\n",
    "        self.G_optimizer=torch.optim.Adam(G.parameters(), lr=lr_generator, betas=(0, 0.9)) \n",
    "\n",
    "        self.reg_param = reg_param\n",
    "        if self.augmentations is not None:\n",
    "            self.x_real = apply_augmentations(x_real, self.augmentations)\n",
    "        else:\n",
    "            self.x_real = x_real\n",
    "\n",
    "    def fit(self, device):\n",
    "        self.G.to(device)\n",
    "        self.D.to(device)\n",
    "        pbar = tqdm(range(self.n_gradient_steps))\n",
    "        for _ in pbar:\n",
    "            self.step(device)\n",
    "            pbar.set_description(\n",
    "                \"G_loss {:1.6e} D_loss {:1.6e} WGAN_GP {:1.6e}\".format(self.losses_history['G_loss'][-1],\n",
    "                                                                       self.losses_history['D_loss'][-1],\n",
    "                                                                       self.losses_history['WGAN_GP'][-1]))\n",
    "\n",
    "    def step(self, device):\n",
    "        \n",
    "        for i in range(self.D_steps_per_G_step):\n",
    "            # generate x_fake\n",
    "            indices = sample_indices(self.x_real.shape[0], self.batch_size)\n",
    "            x_real_batch = self.x_real[indices].to(device)\n",
    "            # torch.no_grad() is a context-manager that disabled gradient calculation for wrapped code.\n",
    "            with torch.no_grad():\n",
    "                x_fake = self.G(batch_size=self.batch_size, n_lags=self.x_real.shape[1], device=device)\n",
    "                if self.augmentations is not None:\n",
    "                    x_fake = apply_augmentations(x_fake, self.augmentations)\n",
    "\n",
    "            D_loss = self.D_train(x_fake, x_real_batch)\n",
    "            if i == 0:\n",
    "                self.losses_history['D_loss'].append(D_loss)\n",
    "        G_loss = self.G_train(device)\n",
    "        self.losses_history['G_loss'].append(G_loss)\n",
    "\n",
    "    def G_train(self, device):\n",
    "\n",
    "        set_requires_grad(self.G, True)\n",
    "\n",
    "        x_fake = self.G(batch_size=self.batch_size, n_lags=self.x_real.shape[1], device=device)\n",
    "        if self.augmentations is not None:\n",
    "            x_fake = apply_augmentations(x_fake, self.augmentations)\n",
    "\n",
    "        self.G.train()\n",
    "        self.G_optimizer.zero_grad()\n",
    "        d_fake = self.D(x_fake)\n",
    "        self.D.train()\n",
    "        # G_loss = self.compute_loss(d_fake, 1)\n",
    "        G_loss = -d_fake.mean()\n",
    "        G_loss.backward()\n",
    "        self.G_optimizer.step()\n",
    "        self.evaluate(x_fake)\n",
    "\n",
    "        set_requires_grad(self.G, False)\n",
    "        # return G_loss.item()\n",
    "        return G_loss.item()\n",
    "\n",
    "    def D_train(self, x_fake, x_real):\n",
    "\n",
    "        set_requires_grad(self.D, True)\n",
    "\n",
    "        self.D.train()\n",
    "        self.D_optimizer.zero_grad()\n",
    "\n",
    "        # Change here\n",
    "        x_real.requires_grad_()\n",
    "        x_fake.requires_grad_()\n",
    "        W1_dist = W1_dist_PathSpace(x_real=x_real,x_fake=x_fake)\n",
    "        total_loss = W1_dist.get_dist(batch_size=self.batch_size)\n",
    "        total_loss.backward()\n",
    "\n",
    "        self.D_optimizer.step()\n",
    "\n",
    "        # Set gradient to False\n",
    "        set_requires_grad(self.D, False)\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'discriminator_type': 'ResFNN', 'hidden_dims': [20, 20], 'input_dim': 3}\n",
      "{'generator_type': 'LSTM', 'hidden_dim': 50, 'n_layers': 2, 'init_fixed': True, 'input_dim': 5, 'output_dim': 1}\n"
     ]
    }
   ],
   "source": [
    "D_config.update(input_dim=wgan_input_dim)\n",
    "print(D_config)\n",
    "G_config.update(output_dim=x_real_dim)\n",
    "print(G_config)\n",
    "\n",
    "D = get_discriminator(**D_config).to(wgan_config['device'])\n",
    "G = get_generator(**G_config).to(wgan_config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SigW1Loss()]\n",
      "[SigW1Loss()]\n"
     ]
    }
   ],
   "source": [
    "test_metrics_train = get_standard_test_metrics(x_real_train)\n",
    "test_metrics_test = get_standard_test_metrics(x_real_test)\n",
    "\n",
    "print(test_metrics_train)\n",
    "print(test_metrics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m WGAN \u001b[38;5;241m=\u001b[39m \u001b[43mWGANTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mx_real\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_real_rolled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtest_metrics_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_metrics_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtest_metrics_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_metrics_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mwgan_config\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m WGAN\u001b[38;5;241m.\u001b[39mfit(device\u001b[38;5;241m=\u001b[39mwgan_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mWGANTrainer.__init__\u001b[0;34m(self, D, G, discriminator_steps_per_generator_step, lr_discriminator, lr_generator, x_real, reg_param, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugmentations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWGANTrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_steps_per_G_step \u001b[38;5;241m=\u001b[39m discriminator_steps_per_generator_step\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD \u001b[38;5;241m=\u001b[39m D\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "WGAN = WGANTrainer(D, G, \n",
    "                   x_real=x_real_rolled, \n",
    "                   test_metrics_train=test_metrics_train,\n",
    "                   test_metrics_test=test_metrics_test,\n",
    "                   **wgan_config\n",
    ")\n",
    "WGAN.fit(device=wgan_config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(WGAN.losses_history['G_loss'], label=\"G_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(WGAN.losses_history['D_loss_fake']) - np.array(WGAN.losses_history['D_loss_real']) + np.array(WGAN.losses_history['WGAN_GP']), label=\"D_loss\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = get_experiment_dir(\"^GSPC\", \"LSTM\", \"ResNN\", 'WGAN', 2024)\n",
    "loss_history = loss_history = os.path.join(experiment_dir, 'LossHistory')\n",
    "os.makedirs(loss_history, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_metrics(WGAN.test_metrics_train, WGAN.losses_history, 'train', locate_dir=loss_history)\n",
    "plot_test_metrics(WGAN.test_metrics_train, WGAN.losses_history, 'test', locate_dir=loss_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siggan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
